{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution in Google Colab\n",
    "You can run this Notebook in Google Colab. Google account required but no local installation, 100% browser based.\n",
    "\n",
    "http://colab.research.google.com/github/floriankilian/sound-delay/blob/plot/colab.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Delay in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b plot https://github.com/floriankilian/sound-delay.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to cloned git repository and show the files.\n",
    "%cd sound-delay\n",
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the # on the next line if you run it local and want the mathplotlib window as a popup to interact with. Does not work on Google Colab.\n",
    "# %matplotlib tk\n",
    "\n",
    "# give the input video file, for exampe: example/video.mp4\n",
    "%run plot_delay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot the two signals\n",
    "fig.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='loudness', line=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=time_video, y=red_intensity, mode='lines', name='red intensity', line=dict(color='red')))\n",
    "\n",
    "# Update the layout, axes properties, and other attributes\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time [seconds]\",\n",
    "    yaxis_title=\"Amplitude\",\n",
    "    yaxis_showticklabels=False,  # Hide y-axis ticks as in the original plot\n",
    "    xaxis_range=[time_min, time_max],\n",
    "    yaxis_range=[amp_min, amp_max],\n",
    "    showlegend=True,\n",
    "    title=\"Comparison of Loudness and Red Intensity over Time\",\n",
    "    xaxis=dict(\n",
    "        title=\"Time [seconds]\",\n",
    "        rangeslider=dict(visible=True),\n",
    "        type='linear'\n",
    "    )  # This line adds the range slider\n",
    ")\n",
    "\n",
    "# Save the plot to the same path/name as the input video\n",
    "output_image_path = video_path[:-3] + 'png'\n",
    "# fig.write_image(output_image_path)  # Requires plotly to be installed with the \"orca\" extra: pip install plotly[orca]\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Gives minor grid for every frame, big grid for every second\n",
    "#\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        ticklen=10,  # Length of major ticks\n",
    "        showgrid=True,  # Gridlines\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate total duration in seconds (round up)\n",
    "total_seconds = int(np.ceil(time_audio[-1]))\n",
    "\n",
    "# Update x-axis for minor ticks representing each frame within a second when zoomed in\n",
    "fig.update_xaxes(\n",
    "    minor_tickmode=\"linear\",\n",
    "    minor_tick0=0,\n",
    "    minor_dtick=1/frame_rate,\n",
    "    minor_ticklen=0,  # Length of minor ticks\n",
    "    minor_showgrid=True,\n",
    "    minor_nticks=frame_rate * total_seconds  # Maximum number of minor ticks\n",
    ")\n",
    "\n",
    "# Display the updated plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BOOM ----\n",
    "\n",
    "# 1. Compute the derivative of the red_intensity\n",
    "red_intensity_derivative = np.diff(red_intensity) / np.diff(time_video)\n",
    "time_video_deriv = time_video[:-1]  # Adjusted time axis for derivative\n",
    "\n",
    "# Normalize the derivative so it's in the same amplitude range as the original signals\n",
    "red_intensity_derivative_norm = (red_intensity_derivative - np.min(red_intensity_derivative)) / (np.max(red_intensity_derivative) - np.min(red_intensity_derivative))\n",
    "red_intensity_derivative_norm = red_intensity_derivative_norm * (np.max(red_intensity) - np.min(red_intensity)) + np.min(red_intensity)\n",
    "\n",
    "# 2. Find the spike in the derivative.\n",
    "first_spike_deriv_red_index = np.argmax(red_intensity_derivative_norm)\n",
    "\n",
    "# 3. After detecting the spike, find the first position where the derivative starts decreasing.\n",
    "first_decreasing_point_after_spike = np.argmax(red_intensity_derivative_norm[first_spike_deriv_red_index:] < red_intensity_derivative_norm[first_spike_deriv_red_index])\n",
    "slowdown_time = time_video_deriv[first_spike_deriv_red_index + first_decreasing_point_after_spike]\n",
    "\n",
    "# ---- SOUND ----\n",
    "\n",
    "# Adjust loudness and time_audio data for calculations to start from slowdown_time\n",
    "start_idx_audio = np.where(time_audio >= slowdown_time)[0][0]\n",
    "adjusted_time_audio = time_audio[start_idx_audio:]\n",
    "adjusted_loudness = loudness[start_idx_audio:]\n",
    "\n",
    "# 1. Compute the derivative of the adjusted loudness signal\n",
    "loudness_derivative = np.diff(adjusted_loudness) / np.diff(adjusted_time_audio)\n",
    "\n",
    "# 2. Determine threshold for detecting a spike in adjusted loudness\n",
    "threshold = 3 * np.std(loudness_derivative)\n",
    "\n",
    "# 3. Detect the first significant spike\n",
    "first_spike_index = np.argmax(loudness_derivative > threshold)\n",
    "\n",
    "# Get the time and value corresponding to the first spike in adjusted loudness\n",
    "first_spike_time = adjusted_time_audio[first_spike_index]\n",
    "first_spike_value = adjusted_loudness[first_spike_index]\n",
    "\n",
    "# Calculate the standard deviation till the point of the max adjusted loudness\n",
    "std_dev_loudness = np.std(adjusted_loudness[:first_spike_index])\n",
    "\n",
    "# ---- PLOTTING AND ANNOTATING ----\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot original signals\n",
    "fig.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=time_video, y=red_intensity, mode='lines', name='Red Intensity', line=dict(color='red')))\n",
    "fig.add_trace(go.Scatter(x=time_video_deriv, y=red_intensity_derivative_norm, mode='lines', name='Normalized Red Intensity Derivative', line=dict(color='rgb(255,100,100)'), visible='legendonly'))\n",
    "\n",
    "# Annotation for the point where derivative starts decreasing\n",
    "fig.add_annotation(\n",
    "    x=slowdown_time,\n",
    "    y=red_intensity_derivative_norm[first_spike_deriv_red_index + first_decreasing_point_after_spike],\n",
    "    text=f'Derivative starts decreasing at {slowdown_time:.2f} s',\n",
    "    showarrow=True,\n",
    "    arrowhead=2\n",
    ")\n",
    "\n",
    "# Annotate the first spike for loudness\n",
    "loudness_ay_offset = -std_dev_loudness * 5  # Offset adjusted based on std dev\n",
    "fig.add_annotation(\n",
    "    x=first_spike_time,\n",
    "    y=first_spike_value,\n",
    "    text=f'First Loudness Spike at {first_spike_time:.2f} s',\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    ax=0,\n",
    "    ay=loudness_ay_offset\n",
    ")\n",
    "\n",
    "# Adding the range slider to the x-axis\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeslider=dict(visible=True),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivative(data, time):\n",
    "    \"\"\"Compute the derivative and return the adjusted time.\"\"\"\n",
    "    derivative = np.diff(data) / np.diff(time)\n",
    "    adjusted_time = time[:-1]\n",
    "    return derivative, adjusted_time\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"Normalize data between 0 and 1 based on its range.\"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# ---- BOOM ----\n",
    "\n",
    "# Derive red_intensity to detect rapid changes which may correspond to visual anomalies (e.g., flash from an explosion).\n",
    "red_intensity_derivative, time_video_deriv = compute_derivative(red_intensity, time_video)\n",
    "\n",
    "# Normalize derivative to align its amplitude range with the original signal, enhancing visualization.\n",
    "red_intensity_derivative_norm = normalize_data(red_intensity_derivative)\n",
    "red_intensity_derivative_norm *= (np.max(red_intensity) - np.min(red_intensity)) + np.min(red_intensity)\n",
    "\n",
    "# Identify the spike in the derivative, which likely indicates the start of the explosion.\n",
    "first_spike_deriv_red_index = np.argmax(red_intensity_derivative_norm)\n",
    "\n",
    "# Detect the point where the increase in intensity begins to slow down post-explosion.\n",
    "first_decreasing_point_after_spike = np.argmax(red_intensity_derivative_norm[first_spike_deriv_red_index:] < red_intensity_derivative_norm[first_spike_deriv_red_index])\n",
    "slowdown_time = time_video_deriv[first_spike_deriv_red_index + first_decreasing_point_after_spike]\n",
    "\n",
    "# ---- SOUND ----\n",
    "\n",
    "# Adjust the audio data to focus on the timeframe after the visual anomaly was detected.\n",
    "start_idx_audio = np.where(time_audio >= slowdown_time)[0][0]\n",
    "adjusted_time_audio, adjusted_loudness = time_audio[start_idx_audio:], loudness[start_idx_audio:]\n",
    "\n",
    "# Derive the loudness to detect rapid changes in sound intensity.\n",
    "loudness_derivative, _ = compute_derivative(adjusted_loudness, adjusted_time_audio)\n",
    "\n",
    "# Define a threshold to detect the significant spike in loudness which likely corresponds to the sound from the explosion.\n",
    "threshold = 3 * np.std(loudness_derivative)\n",
    "first_spike_index = np.argmax(loudness_derivative > threshold)\n",
    "first_spike_time, first_spike_value = adjusted_time_audio[first_spike_index], adjusted_loudness[first_spike_index]\n",
    "\n",
    "# ---- PLOTTING AND ANNOTATING ----\n",
    "\n",
    "# Visualize the original and derived signals for in-depth analysis.\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')),\n",
    "    go.Scatter(x=time_video, y=red_intensity, mode='lines', name='Red Intensity', line=dict(color='red')),\n",
    "])\n",
    "\n",
    "# Annotations to highlight key points of interest in the plot.\n",
    "fig.add_annotation(x=slowdown_time, y=red_intensity_derivative_norm[first_spike_deriv_red_index + first_decreasing_point_after_spike], text=f'Derivative starts decreasing at {slowdown_time:.2f} s', showarrow=True, arrowhead=2)\n",
    "fig.add_annotation(x=first_spike_time, y=first_spike_value, text=f'First Loudness Spike at {first_spike_time:.2f} s', showarrow=True, arrowhead=2, ax=0, ay=-5 * np.std(adjusted_loudness[:first_spike_index]))\n",
    "\n",
    "# Adding the range slider facilitates detailed examination of specific time intervals.\n",
    "fig.update_layout(xaxis=dict(rangeslider=dict(visible=True)))\n",
    "\n",
    "# Add a rectangular shape to highlight the region\n",
    "fig.add_shape(\n",
    "    type=\"rect\",\n",
    "    xref=\"x\",\n",
    "    yref=\"paper\",  # relative to the entire height of the plot\n",
    "    x0=slowdown_time,\n",
    "    x1=first_spike_time,\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    fillcolor=\"grey\",\n",
    "    opacity=0.3,\n",
    "    layer=\"below\",  # place the shape below the traces\n",
    "    line_width=0,\n",
    ")\n",
    "\n",
    "\n",
    "# Display the visual analysis.\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Further Sound Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from scipy.signal import spectrogram\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio signal from the audio_path\n",
    "fs, sig = wavfile.read(audio_path)\n",
    "\n",
    "# Check if the audio signal has multiple channels (e.g., stereo). If so, use only one channel for simplicity.\n",
    "if len(sig.shape) > 1:\n",
    "    sig = sig[:, 0]\n",
    "\n",
    "# Compute the spectrogram using scipy\n",
    "frequencies, times, Sxx = spectrogram(sig, fs=fs, nperseg=4096, noverlap=2048, scaling='density')\n",
    "\n",
    "# Convert amplitude to dB\n",
    "Sxx_db = 10 * np.log10(Sxx)\n",
    "\n",
    "# Plot the original signals and the spectrogram\n",
    "fig_spec = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=('Original Signals', 'Spectrogram'))\n",
    "\n",
    "# Add traces for the original signals\n",
    "fig_spec.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')), row=1, col=1)\n",
    "\n",
    "# Add the spectrogram heatmap\n",
    "fig_spec.add_trace(go.Heatmap(x=times, y=frequencies, z=Sxx_db, colorscale='Viridis'), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig_spec.update_layout(title=\"Original Signals and Spectrogram\")\n",
    "fig_spec.update_yaxes(title_text=\"Frequency (Hz)\", row=2, col=1)\n",
    "fig_spec.update_xaxes(title_text=\"Time (s)\", row=2, col=1)\n",
    "\n",
    "fig_spec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for frequencies below the 'frequency' threshold (e.g., 200 Hz, 300 Hz) - where you would only expect sounds like explosions.\n",
    "# Note: Adjust the 'frequency' variable as needed.\n",
    "frequency = 200\n",
    "mask = frequencies < frequency\n",
    "low_freq_amplitudes = Sxx[mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average amplitude for frequencies below the 'frequency' threshold for each time step.\n",
    "avg_low_freq_amplitude = np.mean(low_freq_amplitudes, axis=0)\n",
    "\n",
    "# Plot the original signals, spectrogram, and the low-frequency amplitude\n",
    "fig_combined = make_subplots(rows=3, cols=1, shared_xaxes=True, \n",
    "                             subplot_titles=('Original Signals', 'Spectrogram', f'Amplitude (<{frequency} Hz)'))\n",
    "\n",
    "# Add traces for the original signals\n",
    "fig_combined.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')), row=1, col=1)\n",
    "fig_combined.add_trace(go.Scatter(x=times, y=avg_low_freq_amplitude, mode='lines', name=f'Amplitude (<{frequency} Hz)', line=dict(color='green')), row=3, col=1)\n",
    "\n",
    "# Add the spectrogram heatmap\n",
    "fig_combined.add_trace(go.Heatmap(x=times, y=frequencies, z=Sxx_db, colorscale='Viridis', showscale=False), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig_combined.update_layout(title=\"Original Signals, Spectrogram, and Low-Frequency Amplitude\")\n",
    "fig_combined.update_yaxes(title_text=\"Frequency (Hz)\", row=2, col=1)\n",
    "fig_combined.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "\n",
    "fig_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the avg_low_freq_amplitude to match loudness amplitude range\n",
    "avg_low_freq_amplitude_norm = (avg_low_freq_amplitude - np.min(avg_low_freq_amplitude)) / (np.max(avg_low_freq_amplitude) - np.min(avg_low_freq_amplitude))\n",
    "avg_low_freq_amplitude_norm = avg_low_freq_amplitude_norm * (np.max(loudness) - np.min(loudness)) + np.min(loudness)\n",
    "\n",
    "# Plot the original signals, spectrogram, and the normalized low-frequency amplitude\n",
    "fig_combined = make_subplots(rows=3, cols=1, shared_xaxes=True, \n",
    "                             subplot_titles=('Original Signals', 'Spectrogram', f'Amplitude (<{frequency} Hz)'))\n",
    "\n",
    "# Add traces for the original signals\n",
    "fig_combined.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')), row=1, col=1)\n",
    "fig_combined.add_trace(go.Scatter(x=times, y=avg_low_freq_amplitude_norm, mode='lines', name=f'Normalized Amplitude (<{frequency} Hz)', line=dict(color='green')), row=3, col=1)\n",
    "\n",
    "# Add the spectrogram heatmap\n",
    "fig_combined.add_trace(go.Heatmap(x=times, y=frequencies, z=Sxx_db, colorscale='Viridis', showscale=False), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig_combined.update_layout(title=\"Original Signals, Spectrogram, and Normalized Low-Frequency Amplitude\")\n",
    "fig_combined.update_yaxes(title_text=\"Frequency (Hz)\", row=2, col=1)\n",
    "fig_combined.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "\n",
    "fig_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BOOM ----\n",
    "\n",
    "# 1. Compute the derivative of the red_intensity\n",
    "red_intensity_derivative = np.diff(red_intensity) / np.diff(time_video)\n",
    "time_video_deriv = time_video[:-1]  # Adjusted time axis for derivative\n",
    "\n",
    "# Normalize the derivative so it's in the same amplitude range as the original signals\n",
    "red_intensity_derivative_norm = (red_intensity_derivative - np.min(red_intensity_derivative)) / (np.max(red_intensity_derivative) - np.min(red_intensity_derivative))\n",
    "red_intensity_derivative_norm = red_intensity_derivative_norm * (np.max(red_intensity) - np.min(red_intensity)) + np.min(red_intensity)\n",
    "\n",
    "# 2. Find the spike in the derivative.\n",
    "first_spike_deriv_red_index = np.argmax(red_intensity_derivative_norm)\n",
    "\n",
    "# 3. After detecting the spike, find the first position where the derivative starts decreasing.\n",
    "first_decreasing_point_after_spike = np.argmax(red_intensity_derivative_norm[first_spike_deriv_red_index:] < red_intensity_derivative_norm[first_spike_deriv_red_index])\n",
    "slowdown_time = time_video_deriv[first_spike_deriv_red_index + first_decreasing_point_after_spike]\n",
    "\n",
    "# ---- SOUND ----\n",
    "\n",
    "# Add loudness_low with avg_low_freq_amplitude_aligned\n",
    "loudness_low = avg_low_freq_amplitude_norm\n",
    "\n",
    "# Adjust loudness and time_audio data for calculations to start from slowdown_time\n",
    "start_idx_audio = np.where(times >= slowdown_time)[0][0]  # Note: using 'times' instead of 'time_audio'\n",
    "adjusted_time_audio = times[start_idx_audio:]\n",
    "adjusted_loudness_low = loudness_low[start_idx_audio:]\n",
    "\n",
    "# 1. Compute the derivative of the adjusted loudness signal\n",
    "loudness_low_derivative = np.diff(adjusted_loudness_low) / np.diff(adjusted_time_audio)\n",
    "\n",
    "# 2. Determine threshold for detecting a spike in adjusted loudness\n",
    "threshold = 3 * np.std(loudness_low_derivative)\n",
    "\n",
    "# 3. Detect the first significant spike\n",
    "first_spike_index = np.argmax(loudness_low_derivative > threshold)\n",
    "\n",
    "# Get the time and value corresponding to the first spike in adjusted loudness\n",
    "first_spike_time = adjusted_time_audio[first_spike_index]\n",
    "first_spike_value = adjusted_loudness_low[first_spike_index]\n",
    "\n",
    "# Calculate the standard deviation till the point of the max adjusted loudness\n",
    "std_dev_loudness_low = np.std(adjusted_loudness_low[:first_spike_index])\n",
    "\n",
    "# ---- PLOTTING AND ANNOTATING ----\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot original signals\n",
    "fig.add_trace(go.Scatter(x=times, y=loudness_low, mode='lines', name=f'Amplitude (<{frequency} Hz)', line=dict(color='green')))  # Note: using 'times' instead of 'time_audio'\n",
    "fig.add_trace(go.Scatter(x=time_video, y=red_intensity, mode='lines', name='Red Intensity', line=dict(color='red')))\n",
    "fig.add_trace(go.Scatter(x=time_audio, y=loudness, mode='lines', name='Loudness', line=dict(color='blue')))  # Note: using 'times' instead of 'time_audio'\n",
    "\n",
    "# Annotation for the point where derivative starts decreasing\n",
    "fig.add_annotation(\n",
    "    x=slowdown_time,\n",
    "    y=red_intensity_derivative_norm[first_spike_deriv_red_index + first_decreasing_point_after_spike],\n",
    "    text=f'Derivative starts decreasing at {slowdown_time:.2f} s',\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    bgcolor=\"rgba(255,255,255,0.7)\",  # Semi-transparent white background\n",
    ")\n",
    "\n",
    "# Annotate the first spike for loudness_low\n",
    "fig.add_annotation(\n",
    "    x=first_spike_time,\n",
    "    y=first_spike_value,\n",
    "    text=f'First Amplitude Spike at {first_spike_time:.2f} s',\n",
    "    showarrow=True,\n",
    "    arrowhead=2,\n",
    "    bgcolor=\"rgba(255,255,255,0.7)\",  # Semi-transparent white background\n",
    ")\n",
    "\n",
    "# Adding the range slider to the x-axis\n",
    "fig.update_layout(\n",
    "    title=\"Comparison of Loudness, Amplitude below Frequency, and Red Intensity Over Time\",\n",
    "    xaxis=dict(\n",
    "        title=\"Time (s)\",\n",
    "        rangeslider=dict(visible=True),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Signal Amplitude\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a rectangular shape to highlight the region\n",
    "fig.add_shape(\n",
    "    type=\"rect\",\n",
    "    xref=\"x\",\n",
    "    yref=\"paper\",  # relative to the entire height of the plot\n",
    "    x0=slowdown_time,\n",
    "    x1=first_spike_time,\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    fillcolor=\"grey\",\n",
    "    opacity=0.3,\n",
    "    layer=\"below\",  # place the shape below the traces\n",
    "    line_width=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Gives minor grid for every frame, big grid for every second\n",
    "#\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        ticklen=10,  # Length of major ticks\n",
    "        showgrid=True,  # Gridlines\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate total duration in seconds (round up)\n",
    "total_seconds = int(np.ceil(time_audio[-1]))\n",
    "\n",
    "# Update x-axis for minor ticks representing each frame within a second when zoomed in\n",
    "fig.update_xaxes(\n",
    "    minor_tickmode=\"linear\",\n",
    "    minor_tick0=0,\n",
    "    minor_dtick=1/frame_rate,\n",
    "    minor_ticklen=0,  # Length of minor ticks\n",
    "    minor_showgrid=True,\n",
    "    minor_nticks=frame_rate * total_seconds  # Maximum number of minor ticks\n",
    ")\n",
    "\n",
    "# Display the updated plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowdown_time, first_spike_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run distance.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Helper for handling video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a mp4 file from a URL \n",
    "You can enter any URL (that links directly to a mp4 file) to download into the file drive of this running colab. Files will not be stored longterm and be deleted after you stop the runtime (or timeout)\n",
    "\n",
    "Use Online Services like TwitterVideoDownloader.com or ttvdl.com (TikTok) to get a .mp4 link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Ask user for the URL\n",
    "url = input(\"Please enter the URL of the MP4 file: \")\n",
    "\n",
    "# Define a suitable filename based on the URL\n",
    "filename = url.split('/')[-1]  # This will take the last part of the URL as the filename. \n",
    "\n",
    "response = requests.get(url)\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Display a success message in the notebook\n",
    "display(HTML(f\"<span style='color: green;'>File downloaded successfully as <b>{filename}</b></span>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming video\n",
    "This Python script enables you to trim a video by specifying the start and end times, creating a new video clip containing only the desired segment. You can use this script to extract specific portions of a video for further editing or sharing.\n",
    "\n",
    "How to Use\n",
    "Input Video File: The script will prompt you to enter the path of the MP4 video file you want to trim. Please provide the full file path, including the file extension (e.g: example/video.mp4).\n",
    "\n",
    "Video Duration: After loading the video, the script will display the total duration of the video in seconds. This information helps you determine the range for trimming.\n",
    "\n",
    "Specify Trimming Times: Enter the start and end times (in seconds) for the portion of the video you want to keep. The script will cut the video from the specified start time to the specified end time.\n",
    "\n",
    "Output Video File: The trimmed video will be saved with a \"-cut\" suffix appended to the original filename. For example, if the original file was named \"video.mp4,\" the trimmed video will be named \"video-cut.mp4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the user for the path of the MP4 file\n",
    "video_path = input(\"Please enter the path of the MP4 file: \")\n",
    "\n",
    "# Load the video clip\n",
    "video_clip = VideoFileClip(video_path)\n",
    "\n",
    "# Give the user information about the duration of the clip\n",
    "video_duration = video_clip.duration\n",
    "print(f\"The video duration is {video_duration:.2f} seconds.\")\n",
    "\n",
    "# Define the start and end times for trimming (in seconds)\n",
    "start_input = input(\"Please enter the start time where you want to cut (default is 0): \")\n",
    "start_time = float(start_input) if start_input else 0  # Start time of the trimmed portion\n",
    "\n",
    "end_input = input(f\"Please enter the end time where you want to cut (default is video duration, {video_duration:.2f}): \")\n",
    "end_time = float(end_input) if end_input else video_duration  # End time of the trimmed portion\n",
    "\n",
    "# Trim the video clip\n",
    "trimmed_clip = video_clip.subclip(start_time, end_time)\n",
    "\n",
    "# Generate the output path with a \"-cut\" suffix\n",
    "output_path = video_path.replace(\".mp4\", f\"-cut_{start_time}_{end_time}.mp4\")\n",
    "\n",
    "# Save the trimmed video with audio\n",
    "trimmed_clip.write_videofile(output_path, codec=\"libx264\")\n",
    "\n",
    "# Close the original video clip\n",
    "video_clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Cropping\n",
    "This Python script allows you to crop a video by specifying the percentage of the frame to cut from the top, bottom, left, and right sides. You can use this script to customize the framing of a video, removing unwanted portions to focus on specific content.\n",
    "\n",
    "How to Use\n",
    "Input Video File: The script will prompt you to enter the path of the MP4 video file you want to edit. Provide the full file path, including the file extension (e.g: example/video.mp4).\n",
    "\n",
    "Percentage to Cut: You will be asked to specify the percentage (0-100) of each side (top, bottom, left, and right) that you want to cut. Higher percentages will result in more cropping, while lower percentages will retain more of the original frame.\n",
    "\n",
    "Output Video File: The edited video will be saved with the specified cropping percentages appended to the filename. For example, if you entered 10% for top, 5% for bottom, 15% for left, and 20% for right, the output file would be named something like original-video-edit_10_5_15_20.mp4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import moviepy.video.fx.all as vfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input video file path\n",
    "input_file_path = input(\"Please enter the path of the MP4 file: \")\n",
    "\n",
    "# Load the video clip\n",
    "video_clip = VideoFileClip(input_file_path)\n",
    "\n",
    "# Get the dimensions of the video frame\n",
    "frame_width, frame_height = video_clip.size\n",
    "\n",
    "# Ask the user for the percentage to cut from each side\n",
    "top_percentage = float(input(\"Enter the percentage to cut from the top (0-100): \"))\n",
    "bottom_percentage = float(input(\"Enter the percentage to cut from the bottom (0-100): \"))\n",
    "left_percentage = float(input(\"Enter the percentage to cut from the left (0-100): \"))\n",
    "right_percentage = float(input(\"Enter the percentage to cut from the right (0-100): \"))\n",
    "\n",
    "# Output video file path with percentages\n",
    "output_file_path = input_file_path.replace(\".mp4\", f\"-edit_{top_percentage}_{bottom_percentage}_{left_percentage}_{right_percentage}.mp4\")\n",
    "\n",
    "# Calculate the pixel values to cut\n",
    "top_cut = int(frame_height * (top_percentage / 100))\n",
    "bottom_cut = int(frame_height * (bottom_percentage / 100))\n",
    "left_cut = int(frame_width * (left_percentage / 100))\n",
    "right_cut = int(frame_width * (right_percentage / 100))\n",
    "\n",
    "# Crop the video clip\n",
    "cropped_clip = video_clip.crop(y1=top_cut, y2=frame_height - bottom_cut, x1=left_cut, x2=frame_width - right_cut)\n",
    "\n",
    "# Write the edited video to the output file\n",
    "cropped_clip.write_videofile(output_file_path, codec=\"libx264\")\n",
    "\n",
    "print(\"Video editing complete. Saved as\", output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound-delay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
